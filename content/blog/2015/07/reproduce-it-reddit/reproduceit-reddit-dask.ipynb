{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='note'>ReproduceIt is a series of articles that reproduce the results from data analysis articles focusing on having open data and open code.</p>\n",
    "\n",
    "On this small article I reproduce the results (not visualization this time) from the reddit user [/u/fhoffa](https://www.reddit.com/user/fhoffa), he posted a nice word cloud visualization of the most common words on some famous subreddits.\n",
    "\n",
    "The reddit post can be found in the data is beautiful subreddit: [Reddit most common words for /r/politics, /r/movies, /r/trees, /r/science](https://www.reddit.com/r/dataisbeautiful/comments/3d9qvj/reddit_most_common_words_for_rpolitics_rmovies/)\n",
    "and the original word cloud was this:\n",
    "![Original Wordcloud](http://i.imgur.com/5Ysd1jE.png)\n",
    "\n",
    "For some context [he mentioned](https://www.reddit.com/r/dataisbeautiful/comments/3d9qvj/reddit_most_common_words_for_rpolitics_rmovies/ct32syr) that he used Google BigQuery and Tableau. The data used was the most recent month available (May 2015) in a recent reddit dump that user [/u/Stuck_In_the_Matrix](https://www.reddit.com/user/Stuck_In_the_Matrix) [made available](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/) in a nice torrent.\n",
    "\n",
    "To reproduce the results I am using [dask](https://github.com/ContinuumIO/dask) which is a nice new project from Continuum Analytics which got a lot of attention in the most recent SciPy. A little disclaimer here: I currently work for Continuum but this post is not sponsored in any way. \n",
    "\n",
    "    Dask enables parallel computing through task scheduling and blocked algorithms. This allows developers to write complex parallel algorithms and execute them in parallel either on a modern multi-core machine or on a distributed cluster.\n",
    "    \n",
    "So basically dask is similar to [Spark](http://spark.apache.org/) in the way of applying transformation to data but it also works amazingly well in a single multi-core machine and reuses battle tested libraries like numpy and pandas.\n",
    "\n",
    "The data I am using is the same data as in the original post wich is 33.46 GB of a JSON new line delimited, each line having one comment with some metadata. All this code ran a my MacBook Pro (Retina, Mid 2014) - 2.8 GHz Intel core i7 - 16 GB 1600 MHz DDR3.\n",
    "\n",
    "Lets jump to the code, as usual some libraries and their versions for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import nltk\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: At the moment I am writing this you need the master version of dask since it includes [a recent feature](https://github.com/ContinuumIO/dask/pull/427) [Matt Rocklin](https://github.com/mrocklin) recently did but it should be available in the next version of dask.\n",
    "\n",
    "**Update**: Dask 0.6.1 has been realeased so you no longer need to use master just `conda install dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `data` variable that points to the big 33 GB file and set the `chunkbytes` to 100 MB, map each item to `json.loads` since I know that each row is a JSON doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = db.from_filenames(\"RC_2015-05\", chunkbytes=100000).map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to spark you can `take` one or more items to see items of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({u'archived': False,\n",
       "  u'author': u'rx109',\n",
       "  u'author_flair_css_class': None,\n",
       "  u'author_flair_text': None,\n",
       "  u'body': u'\\u304f\\u305d\\n\\u8aad\\u307f\\u305f\\u3044\\u304c\\u8cb7\\u3063\\u305f\\u3089\\u8ca0\\u3051\\u306a\\u6c17\\u304c\\u3059\\u308b\\n\\u56f3\\u66f8\\u9928\\u306b\\u51fa\\u306d\\u30fc\\u304b\\u306a',\n",
       "  u'controversiality': 0,\n",
       "  u'created_utc': u'1430438400',\n",
       "  u'distinguished': None,\n",
       "  u'downs': 0,\n",
       "  u'edited': False,\n",
       "  u'gilded': 0,\n",
       "  u'id': u'cqug90g',\n",
       "  u'link_id': u't3_34di91',\n",
       "  u'name': u't1_cqug90g',\n",
       "  u'parent_id': u't3_34di91',\n",
       "  u'removal_reason': None,\n",
       "  u'retrieved_on': 1432703079,\n",
       "  u'score': 4,\n",
       "  u'score_hidden': False,\n",
       "  u'subreddit': u'soccer_jp',\n",
       "  u'subreddit_id': u't5_378oi',\n",
       "  u'ups': 4},)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some simple helper functions that we are going to use as filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_stopwords = lambda x: x not in stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_word = lambda x: re.search(\"^[0-9a-zA-Z]+$\", x) is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it comes the `dask` part which it will look really similar to anyone who has used Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subreddit = data.filter(lambda x: x['subreddit'] == 'movies')\n",
    "bodies = subreddit.pluck('body')\n",
    "words = bodies.map(nltk.word_tokenize).concat()\n",
    "words2 = words.map(lambda x: x.lower())\n",
    "words3 = words2.filter(no_stopwords)\n",
    "words4 = words3.filter(is_word)\n",
    "counts = words4.frequencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `compute` and let dask work. At this moment if you take a look at your process manager you should see something like this: ![Activity Monitor](/images/blog/2015/07/activity-monitor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "values = counts.compute()\n",
    "\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how long the computation took, in this case: around 23 minutes which is about 1.4 GB per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416.4745790958405"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time  # seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the word count done we can just sort the python list and see the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84672"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sort = sorted(values, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'movie', 75848),\n",
       " (u'like', 60028),\n",
       " (u'one', 41682),\n",
       " (u'would', 37354),\n",
       " (u'think', 33539),\n",
       " (u'really', 32494),\n",
       " (u'film', 30931),\n",
       " (u'movies', 30196),\n",
       " (u'people', 30046),\n",
       " (u'good', 27207),\n",
       " (u'see', 25992),\n",
       " (u'time', 22783),\n",
       " (u'get', 21364),\n",
       " (u'much', 20097),\n",
       " (u'even', 19977),\n",
       " (u'deleted', 19464),\n",
       " (u'could', 19338),\n",
       " (u'know', 19275),\n",
       " (u'first', 18435),\n",
       " (u'well', 18155),\n",
       " (u'also', 17935),\n",
       " (u'great', 16729),\n",
       " (u'make', 16668),\n",
       " (u'way', 16494),\n",
       " (u'gt', 16097),\n",
       " (u'still', 15747),\n",
       " (u'pretty', 14808),\n",
       " (u'going', 14649),\n",
       " (u'made', 14630),\n",
       " (u'watch', 13648),\n",
       " (u'http', 13338),\n",
       " (u'actually', 13307),\n",
       " (u'love', 12916),\n",
       " (u'something', 12838),\n",
       " (u'never', 12762),\n",
       " (u'films', 12688),\n",
       " (u'seen', 12642),\n",
       " (u'though', 12576),\n",
       " (u'character', 12395),\n",
       " (u'thought', 12332),\n",
       " (u'say', 12320),\n",
       " (u'man', 12247),\n",
       " (u'scene', 12126),\n",
       " (u'bad', 12065),\n",
       " (u'got', 12004),\n",
       " (u'thing', 11895),\n",
       " (u'lot', 11864),\n",
       " (u'go', 11792),\n",
       " (u'story', 11616),\n",
       " (u'better', 11599),\n",
       " (u'want', 11224),\n",
       " (u'right', 10846),\n",
       " (u'2', 10698),\n",
       " (u'back', 10600),\n",
       " (u'ca', 10434),\n",
       " (u'action', 10286),\n",
       " (u'best', 10261),\n",
       " (u'max', 10066),\n",
       " (u'point', 10031),\n",
       " (u'shit', 9546),\n",
       " (u'sure', 9543),\n",
       " (u'yeah', 9334),\n",
       " (u'look', 9181),\n",
       " (u'new', 9160),\n",
       " (u'said', 9131),\n",
       " (u'probably', 8858),\n",
       " (u'saw', 8832),\n",
       " (u'every', 8811),\n",
       " (u'end', 8776),\n",
       " (u'ever', 8752),\n",
       " (u'guy', 8717),\n",
       " (u'world', 8687),\n",
       " (u'characters', 8605),\n",
       " (u'two', 8567),\n",
       " (u'https', 8521),\n",
       " (u'years', 8478),\n",
       " (u'feel', 8453),\n",
       " (u'things', 8200),\n",
       " (u'many', 8094),\n",
       " (u'part', 8011),\n",
       " (u'maybe', 7917),\n",
       " (u'looks', 7852),\n",
       " (u'always', 7792),\n",
       " (u'mean', 7784),\n",
       " (u'original', 7692),\n",
       " (u'last', 7643),\n",
       " (u'big', 7580),\n",
       " (u'work', 7490),\n",
       " (u'anything', 7480),\n",
       " (u'take', 7478),\n",
       " (u'makes', 7420),\n",
       " (u'fucking', 7415),\n",
       " (u'need', 7286),\n",
       " (u'someone', 7282),\n",
       " (u'different', 7254),\n",
       " (u'little', 7241),\n",
       " (u'mad', 7156),\n",
       " (u'whole', 6882),\n",
       " (u'old', 6782),\n",
       " (u'kind', 6776)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a file with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('r_movies.txt', 'w') as f:\n",
    "    for item in sort:\n",
    "        f.write(\"%s %i\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This was a simple post making some computation on 33 GB of data in a single node using around 20 lines of python. The main objective was to reproduce some of the results other people have made with bigger tools like Google Big Query with simpler but also powerful tools in a single node. Also I used dask for the first time and I was kinda impressed by it.\n",
    "\n",
    "If you have some medium size data a big workstation and dont want to mess up with clusters you should checkout [dask](https://github.com/ContinuumIO/dask). You can also use dask in a cluster but lets save that for a diffent post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
