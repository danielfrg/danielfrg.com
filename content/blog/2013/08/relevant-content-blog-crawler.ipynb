{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We all know that the most important aspect of data science or machine learning is data; with enough quality data you can do everything. Is also not a mistery that the problem of big data is to get that amount of data into a queryable, reportable or undestandable format; now we have a lot of amazing new tools to store that amount of data (casandra, hbase and more) but I still believe that almost nothing beats the fact of collecting a good amount (not necessarily huge, but the more you have the better) but structured data, and there is nothing more structured than SQL.\n",
      "\n",
      "There is a lot of information power in the web and crawl it gives you that power (or is at least the first step), Google does it and I am pretty sure I don't have to say more. I cannot even begin to imagine the amount of work that they do to understand that data. So I created my own mini crawler to crawl what I call relevant content of websites, more specificly blogs, yes I believe blogs and not twitter have a lot of information power, that is why I am writing this in a blog.\n",
      "\n",
      "All I needed was python + some libraries, mainly the readability API. The idea is very simple, get the feed of each blog to get the posts and ask readability to give me the text content of each post. For now this code only works with blogspot and wordpress blogs because is easy to get more than 10 posts from their feed. Also most of the blogs are just on those services.\n",
      "\n",
      "The readability api is beautiful because I dont have to write beautifulsoup code for each site. I tried some implementations of the arc90 readability (javascript and python) without very good results. But if you are looking to pass the 1000 posts per hours of readability API that is the way to go, they just work. But I don't care to wait 3.6 seconds for each post if the content is better.\n",
      "\n",
      "OK, here is the code!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "SQLite"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlalchemy as sql\n",
      "from sqlalchemy.ext.declarative import declarative_base"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "engine = sql.create_engine('sqlite:///blogs.db')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Base = declarative_base()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Post(Base):\n",
      "    __tablename__ = 'post'\n",
      "    \n",
      "    url = sql.Column(sql.String(50), primary_key=True)\n",
      "    date = sql.Column(sql.DateTime)\n",
      "    content = sql.Column(sql.String(10000))\n",
      "\n",
      "    def __init__(self, url, date, content):\n",
      "        self.url = url\n",
      "        self.date = date\n",
      "        self.content = content\n",
      "\n",
      "    def __repr__(self):\n",
      "       return \"<Post('%s','%s')>\" % (self.url, self.date)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Base.metadata.create_all(engine)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Crawler"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import math\n",
      "import time\n",
      "import logging\n",
      "import requests\n",
      "import feedparser\n",
      "import dateutil\n",
      "from datetime import datetime\n",
      "import readability\n",
      "from bs4 import BeautifulSoup\n",
      "import sqlalchemy as sql\n",
      "from sqlalchemy.orm import sessionmaker"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logger = logging.getLogger('crawler')\n",
      "logger.setLevel(logging.DEBUG)\n",
      "handler = logging.FileHandler('crawler.log')\n",
      "f = logging.Formatter(\"%(asctime)s %(message)s\")\n",
      "handler.setFormatter(f)\n",
      "logger.addHandler(handler)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blogs = [\n",
      "    {'url': 'http://mypreciousconfessions.blogspot.com', 'kind': 'blogspot'},\n",
      "    {'url': 'http://cupcakesandcashmere.com', 'kind':'wordpress' }\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Don't ask why those are fashion blogs, I just needed the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_info(blog):\n",
      "    feed = ''\n",
      "    kind = ''\n",
      "    if 'feed' in blog:\n",
      "        feed = blog['feed']\n",
      "        if 'blogger.com' in blog['feed']:\n",
      "            kind = 'blogspot'\n",
      "        elif 'wordpress.com' in blog['feed']:\n",
      "            kind = 'wordpress'\n",
      "        else:\n",
      "            kind = blog['kind']\n",
      "    elif 'url' in blog:\n",
      "        if 'blogspot.com' in blog['url'] or blog['kind'] == 'blogspot':\n",
      "            r = requests.get(blog['url'])\n",
      "            html = r.text\n",
      "            soup = BeautifulSoup(html)\n",
      "            feed = soup.find('link', rel='service.post')['href']\n",
      "            kind = 'blogspot'\n",
      "        elif 'wordpress.com' in blog['url'] or blog['kind'] == 'wordpress':\n",
      "            feed = blog['url'] + '/feed/'\n",
      "            kind = 'wordpress'\n",
      "    return feed, kind"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_posts(blog, limit=10000):\n",
      "    feed, kind = parse_info(blog)\n",
      "    \n",
      "    posts = []\n",
      "    if kind == 'blogspot':\n",
      "        feed = feed + '?max-results=%i' % limit\n",
      "        json_feed = feedparser.parse(feed)\n",
      "        for entry in json_feed['entries']:\n",
      "            date = dateutil.parser.parse(entry['published'])\n",
      "            posts.append((entry['link'], date))\n",
      "    elif kind == 'wordpress':\n",
      "        page = 1\n",
      "        while True and page <= math.ceil(limit / 10):\n",
      "            url = feed + '?paged=%i' % page\n",
      "            r = requests.get(url)\n",
      "            if r.status_code == 200:\n",
      "                json_feed = feedparser.parse(r.text)\n",
      "                for entry in json_feed['entries']:\n",
      "                    if len(posts) < limit:\n",
      "                        date = dateutil.parser.parse(entry['published'])\n",
      "                        posts.append((entry['link'], date))\n",
      "                page += 1\n",
      "            else:\n",
      "                break\n",
      "    return posts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def insert_post(post_link, date, content):\n",
      "    session = Session()\n",
      "    post = Post(post_link, date, content)\n",
      "    session.add(post)\n",
      "    session.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def exists(post_link):\n",
      "    session = Session()\n",
      "    response = session.query(Post).filter(Post.url == post_link).all()\n",
      "    return len(response) == 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def crawl(blogs):\n",
      "    parser = readability.ParserClient('YOUR_READABILITY_API')\n",
      "    for blog in blogs[4:]:\n",
      "        logger.info('---------------------------------------------------------------------------')\n",
      "        posts = get_posts(blog, limit=1000)\n",
      "        n_posts = len(posts)\n",
      "        if 'url' in blog:\n",
      "            logger.info('{0} ({1})'.format(blog['url'], n_posts))\n",
      "        else:\n",
      "            logger.info('{0} ({1})'.format(blog['feed'], n_posts))\n",
      "        logger.info('---------------------------------------------------------------------------')\n",
      "        for i, (post_link, post_date) in enumerate(posts):\n",
      "            if exists(post_link):\n",
      "                logger.info('{0}/{1} Already exists: {2}'.format(i, n_posts, post_link))\n",
      "            else:\n",
      "                parser_response = parser.get_article_content(post_link)\n",
      "                \n",
      "                try:\n",
      "                    soup = BeautifulSoup(parser_response.content['content'])\n",
      "                    content = soup.get_text(\" \", strip=True)\n",
      "                    content = content.replace('\\t', ' ')\n",
      "                    content = content.replace('\"', '')\n",
      "                    insert_post(post_link, post_date, content)\n",
      "                except Exception as e:\n",
      "                    logger.info('{0}/{1} FAIL: {2}'.format(i + 1, n_posts, post_link))\n",
      "                    logger.info(str(e))\n",
      "                else:\n",
      "                    logger.info('{0}/{1} OK: {2}'.format(i + 1, n_posts, post_link))\n",
      "                time.sleep(3.6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That is it! just need to call `crawl(blogs)`\n",
      "\n",
      "Q: I need to crawl faster!\n",
      "\n",
      "A: One easy way to double the speed of crawling is to create another readbility account and [cycle](http://docs.python.org/2/library/itertools.html#itertools.cycle) though the parsers or even better just contact readability ;)\n",
      "\n",
      "Q: Why is this data useful (spoiler of my next post)?\n",
      "\n",
      "A: https://code.google.com/p/word2vec/"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}