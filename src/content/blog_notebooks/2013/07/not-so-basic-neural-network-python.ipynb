{
 "metadata": {
  "name": "nn-no-so-basic"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My [previous post](http://danielfrg.github.io/blog/2013/07/03/basic-neural-network-python/) on implementing a basic Neural Network on python got a lot of attention staying one whole day on HN front page. I was very happy about that but more about the [feedback](https://news.ycombinator.com/item?id=5994851) I got. The community gave me a lot of tips and tricks on how to improve. So now I am presenting an improved version which supports multiple hidden layers,  more optimization options using minibatches and a more maintainable/understandable code (or so I believe)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I kept reading a lot about Neural Networks mainly by watching some videos from [Geoffrey's Hinton Neural Network course](https://class.coursera.org/neuralnets-2012-001/class/index) on coursera. Also reading more on [deeplearning.net](http://deeplearning.net/tutorial/) and trying to read some papers. That last task was definitely the hardest one because of the complexity of the papers. If I cannot even read them I can just imagine how hard is to write them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "About the Neural networks course I didn't like it as much as the Machine Learning course. The main reason is that I had to watch most videos 3 or 4 times before understanding (something). This is definitely my fault because the material is great but it focuses more on the theory (math) which I am not that good at and not on the implementation which I am more interested. But I take it as a learning experience and I will try to finish those videos."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some people criticized (besides my orthography, sorry about my spanglish :P) that I didn't use any cross-validation and that the iris and digits examples are way to simple datasets. I agree that both things are necessary but I am not trying to make this post as a scientific paper with complete benchmarks of different optimization algorithms I leave that to the researchers who have spent years studying neural networks. But in on this case I am using the MNIST dataset to compare.\n",
      "\n",
      "My objective was to learn about them and to have some personal implementation of the algorithm, just to say that I have done it :P"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I was very tempted to use [theano](http://deeplearning.net/software/theano/) and but at the end I decided to do a pure numpy implementation.\n",
      "\n",
      "It is defenitly possible (and not that hard neither that easy) to create a theano implementation and I took some of the ideas from their site such as creating a class for each layer because it makes easier to combine and create different implementations and makes the code more mantainable. But other than the optimization I still cannot see the value of using theano, of course the speed up is important but I thing I prefer the [Numba](http://numba.pydata.org/) approach."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The implementation is very similar to the previous post, the differences are:\n",
      "\n",
      "1. Support for multiple hidden layers. I still train the network using a big 1d-array of weights because that allows to use the scipy optimizations.\n",
      "2. A more \"smart\" random initialization for the arrays.\n",
      "3. Support for mini-batch optimization, which is a must have with bigger datasets.\n",
      "4. Created a Layer class which weights are mapped to slices of the big weight array that is optimized so they are sharing that part of memory. That allows to a more maintainable code and probably to include pre-training in a future.\n",
      "\n",
      "One feature that I think is nice is that when fitting one can actually interrupt (not restart) the ipython kernel and the last weights are going to be saved in the class (`NN.coef_`) so one doesn't have to start the training from nothing, also I added a callback on each epoch so one can for example see the score in a validation dataset."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Basic classes and functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import numpy as np\n",
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Layer(object):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None, activation=None):\n",
      "        if random_state is None:\n",
      "            rnd = np.random.RandomState()\n",
      "        else:\n",
      "            rnd = random_state\n",
      "        \n",
      "        if W is None:\n",
      "            self.W = rnd.uniform(size=(n_out, n_in + 1))\n",
      "        else:\n",
      "            self.W = W\n",
      "        \n",
      "        self.activation = activation\n",
      "        \n",
      "    def output(self, input):\n",
      "        data = np.insert(input, 0, 1, axis=1)\n",
      "        linear_output = np.dot(data, self.W)\n",
      "        return self.activation(linear_output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return np.divide(1., (1 + np.exp(-z)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SigmoidLayer(Layer):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None):\n",
      "        Layer.__init__(self, n_in, n_out, W, random_state, activation=sigmoid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cost function and gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths(weights, weights_meta):\n",
      "    start_pos = 0\n",
      "    for layer in weights_meta:\n",
      "        end_pos = start_pos + layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        start_pos = end_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost(weights, X, y, weights_meta, num_labels):\n",
      "    # Forward\n",
      "    act_prev = np.insert(X, 0, 1, axis=1)\n",
      "    for weight in unpack_weigths(weights, weights_meta):\n",
      "        z = np.dot(act_prev, weight)\n",
      "        activation = sigmoid(z)\n",
      "        act_prev = np.insert(activation, 0, 1, axis=1)\n",
      "    \n",
      "    Y = np.eye(num_labels)[y]\n",
      "    h = activation\n",
      "    costPositive = -Y * np.log(h)\n",
      "    costNegative = (1 - Y) * np.log(1 - h)\n",
      "    J = np.sum(costPositive - costNegative) / X.shape[0]\n",
      "    \n",
      "    return J"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths_inv(weights, weights_meta):\n",
      "    end_pos = len(weights)\n",
      "    for layer in reversed(weights_meta):\n",
      "        start_pos = end_pos - layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        end_pos = start_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost_prime(weights, X, y, weights_meta, num_labels):\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    Deltas = [np.zeros(shape) for shape in weights_meta]\n",
      "    \n",
      "    data = np.insert(X, 0, 1, axis=1)\n",
      "    for i, row in enumerate(data):\n",
      "        # Forward\n",
      "        #row = np.array([row])\n",
      "        act_prev = row\n",
      "        activations = (act_prev, )\n",
      "        for weight in unpack_weigths(weights, weights_meta):\n",
      "            z = np.dot(act_prev, weight)\n",
      "            activation = sigmoid(z)\n",
      "            act_prev = np.append(1, activation)\n",
      "            activations = activations + (act_prev, )\n",
      "        \n",
      "        # Backprop\n",
      "        prev_delta = activations[-1][1:] - Y[i, :].T  # last delta\n",
      "        deltas = (prev_delta, )  # deltas[0] == delta2\n",
      "        for act, weight in zip(reversed(activations[1:-1]), unpack_weigths_inv(weights, weights_meta)):\n",
      "            delta = np.dot(weight, prev_delta)[1:] * (act[1:] * (1 - act[1:])).T\n",
      "            deltas = (delta, ) + deltas\n",
      "            prev_delta = delta\n",
      "\n",
      "        # Accumulate errors\n",
      "        for delta, act, i in zip(deltas, activations[:-1], range(len(Deltas))):\n",
      "            Deltas[i] = Deltas[i] + np.dot(delta[np.newaxis].T, act[np.newaxis]).T\n",
      "    for i in range(len(Deltas)):\n",
      "        Deltas[i] = Deltas[i] / X.shape[0]\n",
      "    return np.concatenate(tuple([D.reshape(-1) for D in Deltas]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Optimization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This class is simulating a `MinibatchOpti` module."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MinibatchOpti(object):\n",
      "    \n",
      "    @staticmethod\n",
      "    def minibatches(X, y=None, batch_size=50, random_state=None):\n",
      "        if random_state is None:\n",
      "            rnd = np.random.RandomState()\n",
      "        elif isinstance(random_state, int):\n",
      "            rnd = np.random.RandomState(random_state)\n",
      "        else:\n",
      "           rnd = random_state\n",
      "\n",
      "        m = X.shape[0]\n",
      "        batch_size = batch_size if batch_size >= 1 else int(math.floor(m * batch_size))\n",
      "        max_batchs = int(math.floor(m / batch_size))\n",
      "        \n",
      "        while True:\n",
      "            random_indices = rnd.choice(np.arange(m), m, replace=False)\n",
      "            for i in range(max_batchs):\n",
      "                batch_indices = np.arange(i * batch_size, (i + 1) * batch_size)\n",
      "                indices = random_indices[batch_indices]\n",
      "                if y is None:\n",
      "                    yield X[indices]\n",
      "                else:\n",
      "                    yield X[indices], y[indices]\n",
      "    @staticmethod\n",
      "    def GD(fun, weights, jac, X, y, options, args=()):\n",
      "        weights -= options['learning_rate'] * jac(weights, X, y, *args)\n",
      "        options['learning_rate'] = options['learning_rate'] * options['learning_rate_decay']\n",
      "    \n",
      "    @staticmethod\n",
      "    def GD_momentum(fun, weights, jac, X, y, options, args=()):\n",
      "        bigjump = options['momentum'] * options['step']\n",
      "        weights -= bigjump\n",
      "        correction = options['learning_rate'] * jac(weights, X, y, *args)\n",
      "        weights -= correction\n",
      "        options['step'] = bigjump + correction\n",
      "        options['learning_rate'] = options['learning_rate'] * options['learning_rate_decay']\n",
      "        options['momentum'] = options['momemtum_decay'] * options['momentum']\n",
      "        \n",
      "    @staticmethod\n",
      "    def RMSPROP(fun, weights, jac, X, y, options, args=()):\n",
      "        gradient = jac(weights, X, y, *args)\n",
      "        options['moving_mean_squared'] = options['decay'] * options['moving_mean_squared'] \\\n",
      "                                         + (1 - options['decay']) * gradient ** 2\n",
      "        weights -= gradient / np.sqrt(options['moving_mean_squared'] + 1e-8)\n",
      "        \n",
      "    @staticmethod\n",
      "    def CG(fun, weights, jac, X, y, options, args=()):\n",
      "        ans = optimize.minimize(fun, weights, jac=jac, method='CG', args=(X, y) + args, options={'maxiter': options['mb_maxiter']})\n",
      "        weights[:] = ans.x\n",
      "        \n",
      "    @staticmethod\n",
      "    def LBFGSB(fun, weights, jac, X, y, options, args=()):\n",
      "        ans = optimize.minimize(fun, weights, jac=jac, method='L-BFGS-B', args=(X, y) + args, options={'maxiter': options['mb_maxiter']})\n",
      "        weights[:] = ans.x\n",
      "    \n",
      "    @staticmethod\n",
      "    def minimize(fun, weights, jac, X, y, method, batch_size=50, tol=1e-6, maxiter=100, args=None, \n",
      "                 verbose=False, options=None, random_state=None, callback=None):\n",
      "        if method == 'GD':\n",
      "            assert 'learning_rate' in options, 'GD needs a learning rate'\n",
      "            if 'learning_rate_decay' not in options:\n",
      "                options['learning_rate_decay'] = 1\n",
      "            if 'momentum' in options:\n",
      "                if 'momemtum_decay' not in options:\n",
      "                    options['momemtum_decay'] = 1\n",
      "                options['step'] = 0\n",
      "                update = MinibatchOpti.GD_momentum\n",
      "            else:\n",
      "                update = MinibatchOpti.GD\n",
      "        elif method == 'RMSPROP':\n",
      "            options['moving_mean_squared'] = 1\n",
      "            update = MinibatchOpti.RMSPROP\n",
      "        elif method == 'CG':\n",
      "            update = MinibatchOpti.CG\n",
      "        elif method == 'L-BFGS-B':\n",
      "            update = MinibatchOpti.LBFGSB\n",
      "        else:\n",
      "            raise Exception('Optimization method not found')\n",
      "\n",
      "        i = 1\n",
      "        prev_cost = 1e8\n",
      "        for _X, _y in MinibatchOpti.minibatches(X, y, batch_size, random_state=random_state):\n",
      "            update(fun, weights, jac, _X, _y, options, args=args)\n",
      "            new_cost = fun(weights, X, y, *args)\n",
      "            diff = new_cost - prev_cost\n",
      "            if np.abs(diff) < tol:\n",
      "                if verbose >= 1:\n",
      "                    print 'Minimum tolerance reached in %i iterations' % i\n",
      "                break\n",
      "            if i >= maxiter:\n",
      "                if verbose >= 1 :\n",
      "                    print 'Maximum number of iterations reached'\n",
      "                break\n",
      "            if verbose >= 2:\n",
      "                print i, newJ    \n",
      "            if callback is not None:\n",
      "                stop = callback(i, weights)\n",
      "                if stop == True:\n",
      "                    break\n",
      "            prev_cost = new_cost\n",
      "            i = i + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The sklearn-stlye class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NN(object):\n",
      "    \n",
      "    def __init__(self, hidden_layers, coef0=None, random_state=None,\n",
      "                 opti_method='GD', batch_size=50, maxiter=100, tol=1e-6, verbose=1, \n",
      "                 opti_options=None, callback=None):\n",
      "        self.hidden_layers = hidden_layers\n",
      "        self.coef_ = None if coef0 is None else np.copy(coef0)\n",
      "        \n",
      "        if random_state is None:\n",
      "            self.rnd = np.random.RandomState()\n",
      "        elif isinstance(random_state, int):\n",
      "            self.rnd = np.random.RandomState(random_state)\n",
      "        else:\n",
      "            self.rnd = random_state\n",
      "        \n",
      "        self.opti_method = opti_method\n",
      "        self.batch_size = batch_size\n",
      "        self.verbose = verbose\n",
      "        self.maxiter = maxiter\n",
      "        self.tol = tol\n",
      "        self.opti_options = {} if opti_options is None else opti_options\n",
      "        self.callback = callback\n",
      "        \n",
      "    def rand_init(self, weights_info, random_state):\n",
      "        w_sizes = []\n",
      "        for layer_info in weights_info:\n",
      "            w_sizes.append(layer_info[0] * layer_info[1])\n",
      "        ans = np.zeros(sum(w_sizes))\n",
      "        \n",
      "        # \"Smart\" random initialization\n",
      "        start_pos = 0\n",
      "        for i, layer in enumerate(weights_info):\n",
      "            end_pos = start_pos + layer[0] * (layer[1])\n",
      "            gap = 4 * np.sqrt(6. / (layer[0] + layer[1]))\n",
      "            ans[start_pos:end_pos] = random_state.uniform(low=-gap, high=gap, size=w_sizes[i])\n",
      "            start_pos = end_pos \n",
      "        return ans\n",
      "    \n",
      "    def predict_proba(self, X):\n",
      "        output = self.layers[0].output(X)\n",
      "        for layer in self.layers[1:]:\n",
      "            output = layer.output(output)\n",
      "        return output\n",
      "   \n",
      "    def predict(self, X):\n",
      "        return self.predict_proba(X).argmax(1)\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        layers = list(self.hidden_layers)  # Copy\n",
      "        layers.insert(0, X.shape[1])\n",
      "        layers.insert(len(layers), len(np.unique(y)))\n",
      "        self.weights_info = [(layers[i] + 1, layers[i + 1]) for i in range(len(layers) - 1)]\n",
      "        self.opti_options = self.opti_options.copy()\n",
      "        \n",
      "        if self.coef_ is None:\n",
      "            self.coef_ = self.rand_init(self.weights_info, self.rnd)\n",
      "\n",
      "        # Unpack the weights and assign them to the layers\n",
      "        self.layers = []\n",
      "        start_pos = 0\n",
      "        for w_info in self.weights_info:\n",
      "            end_pos = start_pos + w_info[0] * (w_info[1])\n",
      "            weight = self.coef_[start_pos:end_pos].reshape((w_info[0], w_info[1]))\n",
      "            self.layers.append(SigmoidLayer(W=weight))\n",
      "            start_pos = end_pos\n",
      "        \n",
      "        args = (self.weights_info, len(np.unique(y)))\n",
      "        MinibatchOpti.minimize(cost, self.coef_, cost_prime, X, y, method=self.opti_method,\n",
      "                               random_state=self.rnd, batch_size=self.batch_size, maxiter=self.maxiter, \n",
      "                               tol=self.tol, args=args, verbose=self.verbose, options=self.opti_options,\n",
      "                               callback=self.callback)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MNINST"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets see how it good it does the using the MINST dataset: 50k rows for training and 10k validations, 748 features.\n",
      "\n",
      "For all the optimization algorithms I am going to use 100 iterations using a batch size of 100. Also I only timed the fitting once because it takes a long time and I am lazy, also the difference between iterations when the times are that long are not that significant.\n",
      "\n",
      "My setup is: Macbook Pro 2.5 GHz Inter Core i5. 16 GB RAM."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, gzip, numpy\n",
      "f = gzip.open('mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, y_train = train_set[0], train_set[1]\n",
      "X_valid, y_valid = valid_set[0], valid_set[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gradient decent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First let's try using a simple gradient decent, the bad part is that one needs to input that [pesky learning rate](http://arxiv.org/abs/1206.1106)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options = {}\n",
      "options['learning_rate'] = 0.3\n",
      "options['learning_rate_decay'] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN([25], opti_method='GD', maxiter=100, opti_options=options, random_state=1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n1 -r1 nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Maximum number of iterations reached\n",
        "1 loops, best of 1: 284 s per loop\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(nn.predict(X_valid), y_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "0.83160000000000001"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "RMSPROP"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To remove the learning rate an easy solution is to use RMSPROP."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options = {}\n",
      "options['decay'] = 0.9"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN([25], opti_method='RMSPROP', maxiter=100, opti_options=options, random_state=1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n1 -r1 nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Maximum number of iterations reached\n",
        "1 loops, best of 1: 297 s per loop\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(nn.predict(X_valid), y_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "0.18629999999999999"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Conjugate gradient"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally my personal favorite is just to use the scipy optimization methods, one has to be careful though because most are for full-batch optimization. But the Conjugate Gradient and L-BFGS-B works on mini-batches. Personally I prefer CG because gave me better results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options = {}\n",
      "options['mb_maxiter'] = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN([25], opti_method='CG', maxiter=100, opti_options=options, random_state=1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n1 -r1 nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Maximum number of iterations reached\n",
        "1 loops, best of 1: 349 s per loop\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(nn.predict(X_valid), y_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "0.86070000000000002"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Comparison"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at the results in a beautiful pandas DataFrame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame([[284, 0.8316], [297, 0.1863], [314, 0.8607]], index=['GD', 'RMSPROP', 'CG'], columns=['Time [s]', 'Accuracy'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Time [s]</th>\n",
        "      <th>Accuracy</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>GD</th>\n",
        "      <td> 284</td>\n",
        "      <td> 0.8316</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>RMSPROP</th>\n",
        "      <td> 297</td>\n",
        "      <td> 0.1863</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>CG</th>\n",
        "      <td> 314</td>\n",
        "      <td> 0.8607</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "         Time [s]  Accuracy\n",
        "GD            284    0.8316\n",
        "RMSPROP       297    0.1863\n",
        "CG            314    0.8607"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even though gradient decent did well I still prefer the scipy optimization methods, maybe because they are just more robust and don't have a learning rate. It takes more time but is not the end of the world. I don't know what is wrong with my RMSPROP implementation I looked at some implementations online and couldn't found the error, if someone sees anything suspicious let me know."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some improvements I would love to do and ideas I want to test:\n",
      "\n",
      "1. Use Numba to speed up the process, in theory having the cost function, its gradient and optimization in different \"well\" defined functions it should be easy.\n",
      "2. Try more optimization methods or why not use an existing implementation because there are many python implementations of these and more algorithms online, just to name a few I found: [python-rl](https://github.com/amarack/python-rl) and [climin](https://github.com/BRML/climin).\n",
      "3. Do pretraining using RBMs: I am just starting to read about them. \n",
      "4. Use minibatches to some point and then switch to bigger minibatches or even full batch optimization.\n",
      "5. People suggested me to use a lookup table for the sigmoid. I tried but the accuracy I got was really bad, I have to check that code again."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I have to admit that I cheated a little bit on the post. Because the neural network supports multiple hidden layers but I only used one!\n",
      "**Why?**: Mainly because it takes a long time to train those networks and I am lazy. Second, I couldn't find a way to make those networks converge without using the scipy advanced optimization methods.\n",
      "\n",
      "From what I understand the problem are the initial weights, and as a side note I spent almost 2 days trying to find a bug while training the MNIST dataset and it was that I started all the layers weights with similar values. The [initial weights are **very** important](http://www.cs.toronto.edu/~gdahl/papers/momentumNesterovDeepLearning.pdf)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I think I have learned enough about neural networks for my personal satisfaction, so this is probably the last iteration of my Neural network. I will definitely keep looking at the updates of NN research but not as almost fulltime as I been doing the past weeks. Is an imposible mission to catch up that amount of information in my free time. Having spent almost a month reading about NN my (obvious) conclusion is that researchers are just scratching the surface of what is possible, and the next years are going to be exciting.\n",
      "\n",
      "If you are looking for a more complete implementation of a deep belif network there is free [python (gpu) implementation](http://www.cs.toronto.edu/~gdahl/) by one person on Geoffrey Hinton group which I haven't look but seams promising and hopefully he will release more code. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}